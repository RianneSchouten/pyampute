{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A custom pipeline with more possibilities\n\nEarlier, we demonstrated how :class:`~pyampute.ampute.MultivariateAmputation` can be integrated in a scikit-learn pipeline (see `A quick example`_ and `Evaluating missing values with grid search and a pipeline`_).\n\nIt may be valuable to understand the impact of missing values in more detail. Therefore, we demonstrate how a ``CustomTransformer`` and ``CustomEstimator`` can be used to do a more thorough analysis. Not only will such analysis gain insights in the statistical problems of missing data (and some imputation methods), but it will also help you to create real-world and realistic missingness scenarios.\n\nAnother example, of a more systematic approach, can be found in `Schouten and Vink (2021)`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Rianne Schouten <https://rianneschouten.github.io/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recap\n\n Given is the following setting (from `Evaluating missing values with grid search and a pipeline`_):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nm = 5\nn = 10000\n\nmean = np.repeat(5, m)\ncor = 0.5\ncov = np.identity(m)\ncov[cov == 0] = cor\nrng = np.random.default_rng()\ncompl_dataset = rng.multivariate_normal(mean, cov, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As amputation parameter settings, we will vary the proportion, the mechanism and the ``score_to_probability_func``. Since in  the latter have to be specified within the same dictionary, we define the parameters for the grid search as follows.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools as it\n\nmechs = [\"MCAR\", \"MAR\", \"MNAR\"]\nfuncs = [\"sigmoid-right\", \"sigmoid-mid\"]\n\nparameters = {\n    \"amputation__prop\": [0.1, 0.5, 0.9],\n    \"amputation__patterns\": [\n        [{\"incomplete_vars\": [0,1], \"mechanism\": mechanism, \"score_to_probability_func\": func}]\n        for mechanism, func in list(it.product(mechs, funcs))]\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A transformer that drops incomplete rows\n\n Previously, we evaluated the ``SimpleImputer`` class from scikit-learn. Another good way to evaluate the effect of missing values, is by analyzing the incomplete dataset directly. Since most prediction and analysis models do not accept missing values, we apply the `dropna` or `listwise deletion` or `complete case analysis` method (all names refer to the same strategy). To allow for integration in a pipeline, we set up a custom ``TransformerMixin``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.base import TransformerMixin\n\nclass DropTransformer(TransformerMixin):\n\n    def __init__(self):\n        super().__init__()\n\n    def fit(self, X, y=None):\n        self.X = X\n        \n        return self\n\n    def transform(self, X, y=None):\n\n        # drop incomplete rows\n        Xp = pd.DataFrame(X)\n        Xdrop = Xp.dropna().to_numpy()\n\t\t\n        return Xdrop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A custom estimator\n\n Almost all, if not all, estimators and evaluation metrics in scikit-learn are aimed at prediction or classification. That is what most people want to do.\n\n However, for evaluating the effect of missing values on your model, it may be good to look further than just the prediction or classification accuracy. In this example, we will focus on the center of the distribution of one feature and evaluate the bias in that distribution.\n\n That could work as follows.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator \n\nclass CustomEstimator(BaseEstimator):\n\n    def __init__(self):\n        super().__init__()\n\n    def fit(self, X, y=None):\n        self.X = X\n        \n        return self\n\n    def predict(self, X):\n\n        # return values of first feature\n        values_used_for_score = X[:,0]\n\t\t\n        return values_used_for_score\n\ndef my_evaluation_metric(y_true, y_pred):\n\n    m1 = np.mean(y_true)\n    m2 = np.mean(y_pred)\n\n    bias = np.abs(m1 - m2)\n\n    return bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An evaluation pipeline\n\n As can be seen, the ``predict`` function returns the first feature of the transformed dataset. The evaluation metric then calculated the mean difference between that feature, and the truth.\n\n In our experiment, the complete dataset is the ground truth and we evaluate the impact of several missing data models (and imputation models) on that truth. \n\n We then run the pipeline twice.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom pyampute.ampute import MultivariateAmputation\nfrom sklearn.metrics import make_scorer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once with the DropTransformer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "steps = [('amputation', MultivariateAmputation()), ('imputation', DropTransformer()), ('estimator', CustomEstimator())]\npipe = Pipeline(steps)\ngrid = GridSearchCV(\n    estimator=pipe,\n    param_grid=parameters,\n    scoring=make_scorer(my_evaluation_metric),\n)\n\ngrid.fit(compl_dataset, np.zeros(len(compl_dataset)))\ngrid.score(compl_dataset, compl_dataset[:,0])\nresults_drop = pd.DataFrame(grid.cv_results_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once with the SimpleImputer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "steps = [('amputation', MultivariateAmputation()), ('imputation', SimpleImputer()), ('estimator', CustomEstimator())]\npipe = Pipeline(steps)\ngrid = GridSearchCV(\n    estimator=pipe,\n    param_grid=parameters,\n    scoring=make_scorer(my_evaluation_metric),\n)\n\ngrid.fit(compl_dataset, np.zeros(len(compl_dataset)))\ngrid.score(compl_dataset, compl_dataset[:,0])\nresults_mean = pd.DataFrame(grid.cv_results_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "res_drop = results_drop[['param_amputation__patterns', 'param_amputation__prop', 'mean_test_score']]\nres_mean = results_mean[['param_amputation__patterns', 'param_amputation__prop', 'mean_test_score']]\n\nres_drop.columns = ['mechanism, func', 'prop', 'score']\nres_mean.columns = ['mechanism, func', 'prop', 'score']\n\nres_drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "res_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What you find here, is that a MCAR mechanism will not affect the center of the distribution of the first feature much, independent of the proportion of incomplete rows. \n\nA MAR mechanism with a sigmoid-right probability function will, on average, remove the right-hand side of the distribution (also, because there is a positive correlation between the observed data and the first feature). Therefore, the larger the proportion, the more bias. However, with a sigmoid-mid probability function, values in the center of the distribution of the first feature are removed, and there is therefore not much effect on the bias. \n\nThe same logic applies to MNAR missingness, but since MNAR missingness does not depend on the size of the correlation between observed data and incomplete data, the bias will be stronger.\n\n`Schouten and Vink (2021)`_ further discuss this topic and the effect of multiple imputation (which can be performed using scikit-learn's IterativeImputer).\n\nSimpleImputer will use the mean of the observed data in the first feature. Therefore, in case there is any bias, that bias will remain. In case there is no bias, mean imputation will distort the correlation structure with other features. But that is another story...\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}