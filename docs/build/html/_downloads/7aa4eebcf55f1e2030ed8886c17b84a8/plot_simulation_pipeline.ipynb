{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Evaluating missing values with a simulation pipeline\n\nGenerating missing values in a complete dataset (we call this `amputation`) may seem like a bizarre thing to do. However, most people who work with all sorts of data will acknowledge that missing data is widespread and can be a severe issue for various types of analyses and models. In order to understand the effect of missing values and to know which missing data methods are appropriate in which situation, we perform simulation studies. And for that, we need amputation. \n\nWith package ``pyampute``, we provide the multivariate amputation methodology proposed by `Schouten et al. (2018)`_. Because our :class:`~pyampute.ampute.MultivariateAmputation` class is built on an sklearn TransformerMixin, it is easy to integrate such evaluation in a larger experiment. Here, we will demonstrate how that works. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Rianne Schouten <https://rianneschouten.github.io/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## General experimental setup\n\n In general, evaluating the effect of missing values is done in four steps:\n\n 1. Generate or import a complete dataset\n 2. Ampute the dataset\n 3. Impute the dataset\n 4. Compare the performance of a model between the datasets in step 1, 2 and 3. \n\n It is often wise to first inspect the effect of amputation (by comparing the datasets in steps 1 and 2) before comparing with the dataset in step 3. Let's get started.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete datasets\n\n A simulation starts with a complete dataset. Make sure that you use a dataset where variables are correlated with each other; otherwise it will not make sense to use weights in a MAR or MNAR mechanism (see `Schouten et al. (2021)`_ for a discussion on this topic). \n\n .. `Schouten et al. (2021)`: https://journals.sagepub.com/doi/full/10.1177/0049124118799376\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nm = 3\nn = 1000\n\nmean = np.repeat(5,m)\ncor = 0.5\ncov = np.identity(m)\ncov[cov == 0] = cor\ncompl_dataset = np.random.multivariate_normal(mean, cov, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multivariate amputation\n\n Vary the parameters of the amputation procedure. As an example, we generate one missing data pattern with missing values in the first two variables. We vary the proportion of incomplete rows and the missingness mechanisms\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parameters = {'amputation_prop': [0.1, 0.5, 0.9], 'amputation_patterns' : [{'incomplete_vars': [0,1], 'mechanism': \"MCAR\"}, {'incomplete_vars': [0,1], 'mechanism': \"MAR\"}, {'incomplete_vars': [0,1], 'mechanism': \"MNAR\"}]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing data methods\n\n `SimpleImputer`_` is a univariate, single imputation method that is commonly used. However, in case of MCAR missingness, it distorts the relation with other variables, and in case of MAR and MNAR missingness it will not resolve issues with shifted variable distributions (see `Van Buuren (2018)`_) It may be better to use a method such as `IterativeImputer`_. \n\n Yet, to demonstrate the working of a simulation pipeline, we will work with SimpleImputer for now.\n\n .. `SimpleImputer`: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n .. `Van Buuren (2018)`: https://stefvanbuuren.name/fimd/\n .. `IterativeImputer`: https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parameters['imputation_strategy'] = [\"mean\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n\n How you wish to evaluate the appropriateness of a missing data method greatly depends on the goal of your model. When you develop a prediction or classification model, you may want to use a standard estimator and evaluation metric.\n\n Here, as an example, we evaluate using principles from statistical theory, where the goal is to find an unbiased and efficient estimate of a population parameter in a sample (that contains missing values). As an easy example, we evaluate an estimate of the mean of a variable, in this case the mean of the first variable. \n\n Therefore, we set up an empty BaseEstimator that returns the values of the first variable. We then design a custom evaluation metric. Since we work with one complete dataset, the true population estimate is the mean of the variable in that dataset. Note, in case we would repeatedly sample a complete dataset, the values of the distribution would become the true population estimate. Here, that would be ``true_mean = 5``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n\nclass CustomEstimator(BaseEstimator):\n\n    def __init__(self):\n        super().__init__()\n\n    def fit(self, X):\n        self.X = X\n        \n        return self\n\n    def predict(self, X):\n        values_first_variable = X[:,0]\n\t\t\n        return values_first_variable\n\ndef my_evaluation_metric(y_true, y_pred):\n\n    bias = np.abs(np.mean(y_true) - np.mean(y_pred))\n\n    return bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Altogether\n\n We then create our pipeline, and run an exhaustive grid search to see the effect of various parameters on the bias of the mean of the first variable.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import GridSearchCV\nfrom pyampute.ampute import MultivariateAmputation\n\nsteps = [('amputation', MultivariateAmputation()), ('imputation', SimpleImputer()), ('estimator', CustomEstimator())]\npipe = Pipeline(steps)\ngrid = GridSearchCV(estimator=pipe, param_grid=parameters, scoring=my_evaluation_metric)\n\ngrid.fit(compl_dataset)\ngrid.score(compl_dataset, compl_dataset[:,0])\npd.DataFrame(grid.cv_results_)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}