{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Single versus multiple imputation\n",
        "\n",
        "By default, the IterativeImputer performs single imputation: a method where\n",
        "every missing value is replaced with one imputed value. The chained character\n",
        "of the method and the possiblity to draw imputation values from the posterior\n",
        "distribution of a Bayesian imputation model allows for the finding of unbiased\n",
        "statistical estimates. However, the disadvantage is that every imputed value is\n",
        "treated as if the value was observed, leading to an imputed dataset that does\n",
        "not reflect the uncertainty that occurs due to the presence of missing values.\n",
        "This makes it hard to find valid statistical inferences because the variance\n",
        "(and standard error) of statistical estimates become too small.\n",
        "\n",
        "An alternative is using the IterativeImputer to perform multiple imputation: a\n",
        "method where every missing value is imputed multiple times. The procedure\n",
        "results in multiple datasets where the observed data is similar in every\n",
        "dataset, but the imputed data is different. All desired steps after imputation\n",
        "are performed on every dataset, such as standardization and other feature\n",
        "engineering steps. The estimation model is also fitted on each of the datasets.\n",
        "\n",
        "One final model is obtained by combining the estimates of each model with\n",
        "Rubin's pooling rules. These rules assume that the parameters of interest are\n",
        "normally distributed which is the case with, for example, estimates of the mean\n",
        "and regression coefficients. Other parameters, such as correlation\n",
        "coefficients need transformation to suit the assumption of normality.\n",
        "If it is not possible to approximate a normal distribution, it is better to use\n",
        "robust summary measures such as medians or ranges instead of using Rubin’s\n",
        "pooling rules. This applies to an estimate like explained variance.\n",
        "\n",
        "In sum, Rubin’s pooling rules are as follows. The overall point estimate after\n",
        "multiple imputation (denoted by Qbar) is the average of all the m point\n",
        "estimates. The variance of the overall point estimate is a combination of\n",
        "so-called within imputation variance (Ubar) and between imputation\n",
        "variance (B). Ubar is the average of the m variances of the m point estimates.\n",
        "Both Qbar and Ubar are corrected with a factor 1 / m to account for sampling\n",
        "variance. The between imputation variance (B) is the sum of the squared\n",
        "difference between Qbar and the m point estimates, corrected with a factor\n",
        "1 / (m – 1). Then, the total variance (T) of the MI overall point estimate is\n",
        "Ubar + B + B/m.\n",
        "\n",
        "In this document we will show how to use the IterativeImputer to perform\n",
        "multiple imputation. In example 1 we show the effect of Rubin’s pooling\n",
        "rules on the variance of regression estimates. Due to the between imputation\n",
        "variance, the standard errors of all regression coefficients are larger with\n",
        "multiple imputation than with single imputation. This allows for valid\n",
        "statistical inference making.\n",
        "\n",
        "In example 2 we show how to set up a prediction model using multiple\n",
        "imputation. We compare two approaches. In one approach, we make predictions for\n",
        "each of the m datasets and combine the m evaluation error metrics into one\n",
        "overall value. In the other approach, we combine the predictions and calculate\n",
        "one evaluation error metric over the averaged predictions. A short simulation\n",
        "study shows that the second approach results in the smallest Mean Squared\n",
        "Error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import ChainedImputer\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "from pyampute import MultivariateAmputation\n",
        "\n",
        "# Make a function that calculates the variance of the beta estimates. This is\n",
        "# necessary because the linear regression model from sklearn does not provide\n",
        "# these values.\n",
        "def calculate_variance_of_beta_estimates(y_true, y_pred, X):\n",
        "\n",
        "    residuals = np.sum((y_true - y_pred) ** 2)\n",
        "    sigma_hat_squared = (1 / (len(y_true) - 2)) * residuals\n",
        "    X_prime_X = np.dot(X.T, X)\n",
        "    covariance_matrix = sigma_hat_squared / X_prime_X\n",
        "    vars = np.diag(covariance_matrix)\n",
        "\n",
        "    return vars\n",
        "\n",
        "\n",
        "# Apply Rubin's pooling rules as follows.\n",
        "# The value of every estimate is the mean of the estimates in each of the m\n",
        "# datasets (Qbar). The variance of these estimates is a combination of the\n",
        "# variance of each of the m estimates (Ubar) and the variance between the m\n",
        "# estimates (B).\n",
        "#\n",
        "# Make a function that calculates Qbar from m estimates\n",
        "def calculate_Qbar(m_estimates):\n",
        "    m = len(m_estimates)\n",
        "    Qbar = 1 / m * np.sum(m_estimates, axis=0)\n",
        "\n",
        "    return Qbar\n",
        "\n",
        "\n",
        "# Make a function that calculates T from m estimates and their variances\n",
        "def calculate_T(m_estimates, m_variances, Qbar):\n",
        "    m = len(m_estimates)\n",
        "    Ubar = 1 / m * np.sum(m_variances, axis=0)\n",
        "    B = 1 / (m - 1) * np.sum((Qbar - m_estimates) ** 2, axis=0)\n",
        "    T = Ubar + B + (B / m)\n",
        "\n",
        "    return T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# EXAMPLE 1. COMPARE STATISTICAL ESTIMATES AND THEIR VARIANCE USING MULTIPLE\n",
        "# IMPUTATION IN A LINEAR REGRESSION MODEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_results_full_dataset(X, y):\n",
        "    # Perform linear regression on full data as a way of comparison\n",
        "    estimator = LinearRegression()\n",
        "    estimator.fit(X, y)\n",
        "    y_predict = estimator.predict(X)\n",
        "\n",
        "    # Save the beta estimates, the variance of these estimates and 1.96 *\n",
        "    # standard error of the estimates. The latter is useful to know the 95%\n",
        "    # confidence interval.\n",
        "    full_coefs = estimator.coef_\n",
        "    full_vars = calculate_variance_of_beta_estimates(y, y_predict, X)\n",
        "    full_errorbar = 1.96 * np.sqrt(full_vars)\n",
        "\n",
        "    return full_coefs, full_vars, full_errorbar\n",
        "\n",
        "\n",
        "def get_results_chained_imputation(X_incomplete, y):\n",
        "    # Impute incomplete data with IterativeImputer using single imputation\n",
        "    # We set n_burn_in at 99 and use only the last imputation\n",
        "    imputer = ChainedImputer(n_burn_in=99, n_imputations=1)\n",
        "    imputer.fit(X_incomplete)\n",
        "    X_imputed = imputer.transform(X_incomplete)\n",
        "\n",
        "    # Perform linear regression on chained single imputed data\n",
        "    # Estimate beta estimates and their variances\n",
        "    estimator = LinearRegression()\n",
        "    estimator.fit(X_imputed, y)\n",
        "    y_predict = estimator.predict(X_imputed)\n",
        "\n",
        "    # Save the beta estimates, the variance of these estimates and 1.96 *\n",
        "    # standard error of the estimates\n",
        "    chained_coefs = estimator.coef_\n",
        "    chained_vars = calculate_variance_of_beta_estimates(y, y_predict, X_imputed)\n",
        "    chained_errorbar = 1.96 * np.sqrt(chained_vars)\n",
        "\n",
        "    return chained_coefs, chained_vars, chained_errorbar\n",
        "\n",
        "\n",
        "def get_results_mice_imputation(X_incomplete, y):\n",
        "    # Impute incomplete data using the IterativeImputer to perform multiple\n",
        "    # imputation. We set n_burn_in at 99 and use only last imputation and\n",
        "    # loop this procedure m times.\n",
        "    m = 5\n",
        "    multiple_imputations = []\n",
        "    for i in range(m):\n",
        "        imputer = ChainedImputer(n_burn_in=99, n_imputations=1, random_state=i)\n",
        "        imputer.fit(X_incomplete)\n",
        "        X_imputed = imputer.transform(X_incomplete)\n",
        "        multiple_imputations.append(X_imputed)\n",
        "\n",
        "    # Perform a model on each of the m imputed datasets\n",
        "    # Estimate the estimates for each model/dataset\n",
        "    m_coefs = []\n",
        "    m_vars = []\n",
        "    for i in range(m):\n",
        "        estimator = LinearRegression()\n",
        "        estimator.fit(multiple_imputations[i], y)\n",
        "        y_predict = estimator.predict(multiple_imputations[i])\n",
        "        m_coefs.append(estimator.coef_)\n",
        "        m_vars.append(\n",
        "            calculate_variance_of_beta_estimates(y, y_predict, multiple_imputations[i])\n",
        "        )\n",
        "\n",
        "    # Calculate the end estimates by applying Rubin's rules.\n",
        "    Qbar = calculate_Qbar(m_coefs)\n",
        "    T = calculate_T(m_coefs, m_vars, Qbar)\n",
        "    mice_errorbar = 1.96 * np.sqrt(T)\n",
        "\n",
        "    return Qbar, T, mice_errorbar\n",
        "\n",
        "\n",
        "# The original multiple imputation procedure as developed under the name\n",
        "# MICE includes all variables in the imputation process; including the output\n",
        "# variable. The reason to do this is that the imputation model should at least\n",
        "# contain the analysis model to result in unbiased estimates. In this function,\n",
        "# we will also include y in the imputation process.\n",
        "def get_results_mice_imputation_includingy(X_incomplete, y):\n",
        "    # Impute incomplete data using the IterativeImputer as a MICEImputer\n",
        "    # Now using the output variable in the imputation loop\n",
        "    m = 5\n",
        "    multiple_imputations = []\n",
        "    for i in range(m):\n",
        "        Xy = np.column_stack((X_incomplete, y))\n",
        "        imputer = ChainedImputer(n_burn_in=99, n_imputations=1, random_state=i)\n",
        "        imputer.fit(Xy)\n",
        "        data_imputed = imputer.transform(Xy)\n",
        "\n",
        "        # We save only the X imputed data because we do not want to use y to\n",
        "        # predict y later on.\n",
        "        X_imputed = data_imputed[:, :-1]\n",
        "        multiple_imputations.append(X_imputed)\n",
        "\n",
        "    # Perform linear regression on mice multiple imputed data\n",
        "    # Estimate beta estimates and their variances\n",
        "    m_coefs = []\n",
        "    m_vars = []\n",
        "    for i in range(m):\n",
        "        estimator = LinearRegression()\n",
        "        estimator.fit(multiple_imputations[i], y)\n",
        "        y_predict = estimator.predict(multiple_imputations[i])\n",
        "        m_coefs.append(estimator.coef_)\n",
        "        m_vars.append(\n",
        "            calculate_variance_of_beta_estimates(y, y_predict, multiple_imputations[i])\n",
        "        )\n",
        "\n",
        "    # Calculate the end estimates by applying Rubin's rules.\n",
        "    Qbar = calculate_Qbar(m_coefs)\n",
        "    T = calculate_T(m_coefs, m_vars, Qbar)\n",
        "    mice_errorbar = 1.96 * np.sqrt(T)\n",
        "\n",
        "    return Qbar, T, mice_errorbar\n",
        "\n",
        "\n",
        "# Now lets run all these imputation procedures.\n",
        "# We use the Boston dataset and analyze the outcomes of the beta coefficients\n",
        "# and their standard errors. We standardize the data before running the\n",
        "# procedure to be able to compare the coefficients. We run the procedure for\n",
        "# MCAR missingness only.\n",
        "#\n",
        "# Loading the data\n",
        "dataset = load_boston()\n",
        "X_full, y = dataset.data, dataset.target\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_full)\n",
        "y_scaled = stats.zscore(y)\n",
        "\n",
        "# Start the procedure\n",
        "print(\"Executing Example 1 MCAR Missingness...\")\n",
        "\n",
        "# First, make the data incomplete with a MCAR mechanism.\n",
        "am_MCAR = MultivariateAmputation(mechanisms=\"MCAR\")\n",
        "Boston_X_incomplete_MCAR = am_MCAR(X_scaled)\n",
        "\n",
        "# Second, run all the imputation procedures as described above.\n",
        "full_coefs, full_vars, full_errorbar = get_results_full_dataset(X_scaled, y_scaled)\n",
        "chained_coefs, chained_vars, chained_errorbar = get_results_chained_imputation(\n",
        "    Boston_X_incomplete_MCAR, y_scaled\n",
        ")\n",
        "mice_coefs, mice_vars, mice_errorbar = get_results_mice_imputation(\n",
        "    Boston_X_incomplete_MCAR, y_scaled\n",
        ")\n",
        "mice_y_coefs, mice_y_vars, mice_y_errorbar = get_results_mice_imputation_includingy(\n",
        "    Boston_X_incomplete_MCAR, y_scaled\n",
        ")\n",
        "\n",
        "# Combine the results from the four imputation procedures.\n",
        "coefs = (full_coefs, chained_coefs, mice_coefs, mice_y_coefs)\n",
        "vars = (full_vars, chained_vars, mice_vars, mice_y_vars)\n",
        "errorbars = (full_errorbar, chained_errorbar, mice_errorbar, mice_y_errorbar)\n",
        "\n",
        "# And plot the results\n",
        "n_situations = 4\n",
        "n = np.arange(n_situations)\n",
        "n_labels = [\"Full Data\", \"Chained Imputer\", \"Mice Imputer\", \"Mice Imputer with y\"]\n",
        "colors = [\"r\", \"orange\", \"b\", \"purple\"]\n",
        "width = 0.3\n",
        "plt.figure(figsize=(24, 32))\n",
        "\n",
        "plt1 = plt.subplot(211)\n",
        "for j in n:\n",
        "    plt1.bar(\n",
        "        np.arange(len(coefs[j])) + (3 * j * (width / n_situations)),\n",
        "        coefs[j],\n",
        "        width=width,\n",
        "        color=colors[j],\n",
        "    )\n",
        "plt.legend(n_labels)\n",
        "\n",
        "plt2 = plt.subplot(212)\n",
        "for j in n:\n",
        "    plt2.bar(\n",
        "        np.arange(len(errorbars[j])) + (3 * j * (width / n_situations)),\n",
        "        errorbars[j],\n",
        "        width=width,\n",
        "        color=colors[j],\n",
        "    )\n",
        "\n",
        "plt1.set_title(\"MCAR Missingness\")\n",
        "plt1.set_ylabel(\"Beta Coefficients\")\n",
        "plt2.set_ylabel(\"Standard Errors\")\n",
        "plt1.set_xlabel(\"Features\")\n",
        "plt2.set_xlabel(\"Features\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# EXAMPLE 2. SHOW MULTIPLE IMPUTATION IN A PREDICTION CONTEXT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# In this example, we show how to apply multiple imputation in a train/test\n",
        "# situation. There are two approaches to get the end result of the prediction\n",
        "# model. In approach 1 you calculate the evaluation metric for every i in m and\n",
        "# later average these values. In approach 2 you average the predictions of\n",
        "# every i in m and then calculate the evaluation metric. We test both\n",
        "# approaches.\n",
        "#\n",
        "# Apply the regression model on the full dataset as a way of comparison.\n",
        "def get_results_full_data(X_train, X_test, y_train, y_test):\n",
        "    # Standardize data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Perform estimation and prediction\n",
        "    estimator = LinearRegression()\n",
        "    estimator.fit(X_train_scaled, y_train)\n",
        "    y_predict = estimator.predict(X_test_scaled)\n",
        "    mse_full = mse(y_test, y_predict)\n",
        "\n",
        "    return mse_full\n",
        "\n",
        "\n",
        "# Use the ChainedImputer as a single imputation procedure.\n",
        "def get_results_single_imputation(X_train, X_test, y_train, y_test):\n",
        "    # Apply imputation\n",
        "    imputer = ChainedImputer(n_burn_in=99, n_imputations=1, random_state=0)\n",
        "    X_train_imputed = imputer.fit_transform(X_train)\n",
        "    X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "    # Standardize data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "    X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "    # Perform estimation and prediction\n",
        "    estimator = LinearRegression()\n",
        "    estimator.fit(X_train_scaled, y_train)\n",
        "    y_predict = estimator.predict(X_test_scaled)\n",
        "    mse_single = mse(y_test, y_predict)\n",
        "\n",
        "    return mse_single\n",
        "\n",
        "\n",
        "# Now use the IterativeImputer to perform multiple imputation by looping over\n",
        "# i in m. Approach 1: pool the mse values of the m datasets.\n",
        "def get_results_multiple_imputation_approach1(X_train, X_test, y_train, y_test):\n",
        "    m = 5\n",
        "    multiple_mses = []\n",
        "    for i in range(m):\n",
        "        # Fit the imputer for every i in im\n",
        "        # Be aware that you fit the imputer on the train data\n",
        "        # And apply to the test data\n",
        "        imputer = ChainedImputer(n_burn_in=99, n_imputations=1, random_state=i)\n",
        "        X_train_imputed = imputer.fit_transform(X_train)\n",
        "        X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "        # Perform the steps you wish to take before fitting the estimator\n",
        "        # Such as standardization.\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "        X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "        # Finally fit the estimator and calculate the error metric for every i\n",
        "        # in m. Save all error metric values.\n",
        "        estimator = LinearRegression()\n",
        "        estimator.fit(X_train_scaled, y_train)\n",
        "        y_predict = estimator.predict(X_test_scaled)\n",
        "        mse_approach1 = mse(y_test, y_predict)\n",
        "        multiple_mses.append(mse_approach1)\n",
        "\n",
        "    # Average the error metric values over the m loops to get a final result.\n",
        "    mse_approach1 = np.mean(multiple_mses, axis=0)\n",
        "\n",
        "    return mse_approach1\n",
        "\n",
        "\n",
        "# Approach 2: We average the predictions of the m datasets and then calculate\n",
        "# the error metric.\n",
        "def get_results_multiple_imputation_approach2(X_train, X_test, y_train, y_test):\n",
        "    m = 5\n",
        "    multiple_predictions = []\n",
        "    for i in range(m):\n",
        "        # Fit the imputer for every i in m\n",
        "        # Be aware that you fit the imputer on the train data\n",
        "        # And apply to the test data\n",
        "        imputer = ChainedImputer(n_burn_in=99, n_imputations=1, random_state=i)\n",
        "        X_train_imputed = imputer.fit_transform(X_train)\n",
        "        X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "        # Perform the steps you wish to take before fitting the estimator\n",
        "        # Such as standardization\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "        X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "        # Finally fit the estimator and calculate the predictions for every i\n",
        "        # in m. Save the predictions.\n",
        "        estimator = LinearRegression()\n",
        "        estimator.fit(X_train_scaled, y_train)\n",
        "        y_predict = estimator.predict(X_test_scaled)\n",
        "        multiple_predictions.append(y_predict)\n",
        "\n",
        "    # Average the predictions over the m loops\n",
        "    # Then calculate the error metric.\n",
        "    predictions_average = np.mean(multiple_predictions, axis=0)\n",
        "    mse_approach2 = mse(y_test, predictions_average)\n",
        "\n",
        "    return mse_approach2\n",
        "\n",
        "\n",
        "def perform_simulation(dataset, X_incomplete, nsim=10):\n",
        "    X_full, y = dataset.data, dataset.target\n",
        "    outcome = []\n",
        "\n",
        "    # Start a simulation process that executes the process nsim times.\n",
        "    for j in np.arange(nsim):\n",
        "        # First, split the data in train and test dataset.\n",
        "        train_indices, test_indices = train_test_split(\n",
        "            np.arange(X_full.shape[0]), random_state=j\n",
        "        )\n",
        "        X_incomplete_train = X_incomplete[train_indices]\n",
        "        X_full_train = X_full[train_indices]\n",
        "        X_incomplete_test = X_incomplete[test_indices]\n",
        "        X_full_test = X_full[test_indices]\n",
        "        y_train = y[train_indices]\n",
        "        y_test = y[test_indices]\n",
        "\n",
        "        # Second, perform the imputation procedures and calculation of the\n",
        "        # error metric for every one of the four situations.\n",
        "        mse_full = get_results_full_data(X_full_train, X_full_test, y_train, y_test)\n",
        "        mse_single = get_results_single_imputation(\n",
        "            X_incomplete_train, X_incomplete_test, y_train, y_test\n",
        "        )\n",
        "        mse_approach1 = get_results_multiple_imputation_approach1(\n",
        "            X_incomplete_train, X_incomplete_test, y_train, y_test\n",
        "        )\n",
        "        mse_approach2 = get_results_multiple_imputation_approach2(\n",
        "            X_incomplete_train, X_incomplete_test, y_train, y_test\n",
        "        )\n",
        "\n",
        "        # Save the outcome of every simulation round\n",
        "        outcome.append((mse_full, mse_single, mse_approach1, mse_approach2))\n",
        "\n",
        "    # Return the mean and standard deviation of the nsim outcome values\n",
        "    return np.mean(outcome, axis=0), np.std(outcome, axis=0)\n",
        "\n",
        "\n",
        "# Execute the simulation\n",
        "print(\"Executing Example 2 MCAR Missingness...\")\n",
        "\n",
        "# Generate missing values with a MCAR mechanism\n",
        "am_MCAR = MultivariateAmputation(mechanisms=\"MCAR\")\n",
        "Boston_X_incomplete_MCAR = am_MCAR(X_scaled)\n",
        "\n",
        "# Perform the simulation\n",
        "mse_means, mse_std = perform_simulation(load_boston(), Boston_X_incomplete_MCAR, nsim=2)\n",
        "\n",
        "# Plot results\n",
        "n_situations = 4\n",
        "n = np.arange(n_situations)\n",
        "n_labels = [\n",
        "    \"Full Data\",\n",
        "    \"Single Imputation\",\n",
        "    \"MI Average MSE\",\n",
        "    \"MI Average Predictions\",\n",
        "]\n",
        "colors = [\"r\", \"orange\", \"green\", \"yellow\"]\n",
        "\n",
        "plt.figure(figsize=(24, 12))\n",
        "ax1 = plt.subplot(111)\n",
        "for j in n:\n",
        "    ax1.barh(\n",
        "        j, mse_means[j], xerr=mse_std[j], color=colors[j], alpha=0.6, align=\"center\"\n",
        "    )\n",
        "\n",
        "ax1.set_title(\"MCAR Missingness\")\n",
        "ax1.set_yticks(n)\n",
        "ax1.set_xlabel(\"Mean Squared Error\")\n",
        "ax1.invert_yaxis()\n",
        "ax1.set_yticklabels(n_labels)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
